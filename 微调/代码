# 部署
# from modelscope import snapshot_download, AutoTokenizer
# from transformers import AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq
# import torch
# # 在modelscope上下载Qwen模型到本地目录下
# model_dir = snapshot_download("Qwen/Qwen-7B-Chat", cache_dir="./", revision="master")
# tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen-7B-Chat", use_fast=False, trust_remote_code=True)
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="auto", torch_dtype=torch.bfloat16,trust_remote_code=True)
# response, history = model.chat(tokenizer, "你好", history=None)
# print(response)
# response, history = model.chat(tokenizer, "帮我写一首有关水手在大海上捕鲸的英文短诗", history=history)
# print(response)
# response, history = model.chat(tokenizer, "给这个短诗起一个英文标题", history=history)
# print(response)
#注意：计算history是非常慢的，用CPU跑非常辛苦


#目前的问题就是数据集是要等负责的同学弄好的，这里是用一个问诊和医生回应的数据集做的训练，跟最终的要求是有区别的，但是按照demo的要求应该是可以了
#其次就是模型用的是Qwen2-7B-Instruct而不是Qwen/Qwen-7B-Chat，但是我觉得问题不大，前者体积大，但是据说效果好

import json
import pandas as pd
from datasets import Dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer
import torch

f = open(r"C:\Users\abc\Desktop\样例_内科5000-6000(1).csv", "r", encoding='gbk')
ls = []
for line in f:
    line = line.replace("\n", "")
    ls.append(line.split(","))
f.close()

fw = open("a.json", "w", encoding='utf-8')
for i in range(1, len(ls)):
    ls[i] = dict(zip(ls[0], ls[i]))
a = json.dumps(ls[1:], sort_keys=True, indent=4, ensure_ascii=False)
fw.write(a)
fw.close()

df = pd.read_json("a.json")
ds = Dataset.from_pandas(df)
print(ds[:5])

tokenizer = AutoTokenizer.from_pretrained('qwen/Qwen-7B-Chat', use_fast=False)
print(tokenizer)

def process_func(example):
    MAX_LENGTH = 384
    input_ids, attention_mask, labels = [], [], []
    instruction = tokenizer(
        f"<|im_start|>system\n你是一个医疗问答专家，你会接收到某个诊疗室的一段病患自我描述病情的文本，请输出每一个病患的主诉，现病史，辅助检查<|im_end|>\n<|im_start|>user\n{example['instruction'] + str(example['input'])}<|im_end|>\n<|im_start|>assistant\n",
        add_special_tokens=False
    )
    response = tokenizer(f"{example['output']}", add_special_tokens=False)

    input_ids = instruction["input_ids"] + response["input_ids"] + [tokenizer.pad_token_id]
    attention_mask = instruction["attention_mask"] + response["attention_mask"] + [1]
    labels = [-100] * len(instruction["input_ids"]) + response["input_ids"] + [tokenizer.pad_token_id]
    if len(input_ids) > MAX_LENGTH:
        input_ids = input_ids[:MAX_LENGTH]
        attention_mask = attention_mask[:MAX_LENGTH]
        labels = labels[:MAX_LENGTH]
    return {
        "input_ids": input_ids,
        "attention_mask": attention_mask,
        "labels": labels
    }

tokenized_id = ds.map(process_func, remove_columns=ds.column_names)
print(tokenized_id)
print(tokenizer.decode(tokenized_id[0]['input_ids']))
print(tokenizer.decode(list(filter(lambda x: x != -100, tokenized_id[1]["labels"]))))

model = AutoModelForCausalLM.from_pretrained('/root/autodl-tmp/qwen/Qwen2-7B-Instruct', torch_dtype=torch.float32)
model = model.to('cpu')
model.enable_input_require_grads()

config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    inference_mode=False,
    r=8,
    lora_alpha=32,
    lora_dropout=0.1
)
model = get_peft_model(model, config)
model.print_trainable_parameters()

args = TrainingArguments(
    output_dir="./output/Qwen2_7B_instruct_lora_cpu",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    logging_steps=10,
    num_train_epochs=3,
    save_steps=10,
    learning_rate=1e-4,
    save_on_each_node=True,
    gradient_checkpointing=True
)

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=tokenized_id,
    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),
)

trainer.train()

tokenizer = AutoTokenizer.from_pretrained('/root/autodl-tmp/qwen/Qwen2-7B-Instruct')
model = AutoModelForCausalLM.from_pretrained('/root/autodl-tmp/qwen/Qwen2-7B-Instruct', torch_dtype=torch.float32).eval()
model = model.to('cpu')

prompt = "你是谁？"
messages = [
    {"role": "user", "content": "你是一个医疗问答专家，你会接收到某个诊疗室的一段病患自我描述病情的文本，请输出每一个病患的主诉，现病史，辅助检查"},
    {"role": "user", "content": prompt}
]
inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_tensors="pt", return_dict=True)

gen_kwargs = {"max_length": 2500, "do_sample": True, "top_k": 1}
with torch.no_grad():
    outputs = model.generate(**inputs, **gen_kwargs)
    outputs = outputs[:, inputs['input_ids'].shape[1]:]
    print(tokenizer.decode(outputs[0], skip_special_tokens=True))